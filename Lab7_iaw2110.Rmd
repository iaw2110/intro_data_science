---
title: "Lab 7 (Bayesian Estimation and MCMC)"
author: "Ivan Wolansky, iaw2110"
date: "4/19/2019"
output: pdf_document
---

# Instructions 
Make sure that you upload the PDF (or HTML) output after you have knitted the file. The files you upload to the Canvas page should be updated with commands you provide to answer each of the questions below. You can edit this file directly to produce your final solutions.     

## Goals

This lab has two goals. The first goal is to use the **Accept-Reject** algorithm to simulate from a mixture of two normals.  The second goal is to utilize Bayesian methods and the the famous **Markov Chain Monte Carlo** algorithm to estimate the mixture parameter $\delta$. 

# Background: (Mixture)   

A mixture distribution is the probability distribution of a random variable that is derived from a collection of other random variables (Wiki).  In our case we consider a mixture of two normal distributions. Here we assume that our random variable is governed by the probability density $f(x)$, defined by
\begin{align*}
f(x)&=f(x;\mu_1,\sigma_1,\mu_2,\sigma_2,\delta)\\
&=\delta f_1(x;\mu_1,\sigma_1)+(1-\delta)f_2(x;\mu_2,\sigma_2)\\
 &=\delta \frac{1}{\sqrt{2 \pi \sigma_1^2}}\exp{-\frac{1}{2\sigma_1^2}(x-\mu_1)^2}+(1-\delta) \frac{1}{\sqrt{2 \pi \sigma_2^2}}\exp{-\frac{1}{2\sigma_2^2}(x-\mu_2)^2}, 
\end{align*}
where  $-\infty<x<\infty$ and the parameter space is defined by $-\infty < \mu_1,\mu_2 <\infty$, $\sigma_1,\sigma_2 >0$, and $0\leq\delta\leq1$.  The **mixture parameter** $\delta$ governs how much mass gets placed on the first distribution $f(x;\mu_1,\sigma_1)$ and the complement of $\delta$ governs how much mass gets placed on the other distribution $f_2(x;\mu_2,\sigma_2)$.  

To further motivate this setting, consider simulating $n=10,000$ heights from the population of both males and females.  Assume that males are distributed normal with mean $\mu_1=70\text{[in]}$ and standard deviation $\sigma_1=3\text{[in]}$ and females are distributed normal with mean $\mu_2=64\text{[in]}$ and standard deviation $\sigma_2=2.5\text{[in]}$.  Also assume that each distribution contributes equal mass, i.e., set the mixture parameter to $\delta=.5$.  The distribution of males is governed by 
\[
f_1(x;\mu_1,\sigma_1)=\frac{1}{\sqrt{2 \pi \sigma_1^2}}\exp{-\frac{1}{2\sigma_1^2}(x-\mu_1)^2}, \ \ \ -\infty<x<\infty,
\]
and the distribution of females is governed by
\[
f_2(x;\mu_2,\sigma_2)=\frac{1}{\sqrt{2 \pi \sigma_2^2}}\exp{-\frac{1}{2\sigma_2^2}(x-\mu_2)^2}, \ \ \ -\infty<x<\infty.
\]

Below shows the pdf of $f_1(x;\mu_1,\sigma_1)$, $f_2(x;\mu_2,\sigma_2)$ and the mixture $f(x)$ all on the same plot. 

```{r}
x <- seq(45,90,by=.05)
n.x <- length(x)
f_1 <- dnorm(x,mean=70,sd=3)
f_2 <- dnorm(x,mean=64,sd=2.5)
f <- function(x) {
  return(.5*dnorm(x,mean=70,sd=3) + .5*dnorm(x,mean=64,sd=2.5))
  }

plot_df <-  data.frame(x=c(x,x,x),
                       Density=c(f_1,f_2,f(x)),
                       Distribution=c(rep("Male",n.x),rep("Female",n.x),rep("Mixture",n.x))
                       )

library(ggplot2)
ggplot(data = plot_df) +
   geom_line(mapping = aes(x = x, y = Density,color=Distribution))+
   labs(title = "Mixture of Normals")  
```

## Part I: Simulating a Mixture of Normals   

The first goal is to simulate from the mixture distribution 
\[
\delta f_1(x;\mu_1,\sigma_1)+(1-\delta)f_2(x;\mu_2,\sigma_2),
\]
where $\mu_1=70,\sigma_1=3,\mu_2=64,\sigma_2=2.5,\delta=2.5$.  We use the accept-reject algorithm to accomplish this task. 

First we must choose the "easy to simulate" distribution $g(x)$.  For this problem choose $g(x)$ to be a Cauchy distribution centered at 66 with scale parameter $7$.  

```{r}
g <- function(x) {
  s=7
  l=66
  return(1/(pi*s*(1+((x-l)/s)^2)))
  }
```

**Perform the following tasks**

1) Identify a **suitable** value of $alpha$ such that your envelope function $e(x)$ satisfies
\[
f(x) \leq e(x) = g(x)/\alpha, \ \ \text{where} \ \ 0<\alpha<1.
\]
Note that you must choose $\alpha$ so that $e(x)$ is close to $f(x)$. There is not one unique solution to this problem. The below plot shows how $\alpha=.20$ creates an envelope function that is too large. Validate your choice of $alpha$ with with a graphic similar to below.  


```{r}
# Choose alpha
alpha <- .20

# Define envelope e(x)
e <- function(x) {
  return(g(x)/alpha)
  }

# Plot
x.vec <- seq(30,100,by=.1)
ggplot() +
   geom_line(mapping = aes(x = x.vec, y = f(x.vec)),col="purple")+
   geom_line(mapping = aes(x = x.vec, y = e(x.vec)),col="green")

# Is g(x)>f(x)? 
all(e(x.vec)>f(x.vec))
```

**Solution**
```{r}
# Choose alpha
alpha <- .44

# Define envelope e(x)
e <- function(x) {
  return(g(x)/alpha)
  }

# Plot
x.vec <- seq(30,100,by=.1)
ggplot() +
   geom_line(mapping = aes(x = x.vec, y = f(x.vec)),col="purple")+
   geom_line(mapping = aes(x = x.vec, y = e(x.vec)),col="green")

# Is g(x)>f(x)? 
all(e(x.vec)>f(x.vec))
```

2) Write a function named **r.norm.mix()** that simulates **n.samps** from the normal-mixture $f(x)$.  To accomplish this task you will wrap a function around the accept-reject algorithm from the lecture notes.  Also include the acceptance rate, i.e., how many times did the algorithm accept a draw compared to the total number of trials performed.  Your function should return a list of two elements: (i) the simulated vector mixture and (ii) the proportion of accepted cases. Run your function **r.norm.mix()** to simulate 10,000 cases and display the first 20 values. What's the proportion of accepted cases? Compare this number to your chosen $\alpha$ and comment on the result. The code below should help you get started. 

**Solution**

```{r}
cauchy.sim <- function(a, scale, location) {
  z <- runif(a)
  return(ifelse((z<0|z>1), 0, scale * tan(pi * (z-1/2)) + location))
}

r.norm.mix <- function(n.samps) {
  n <- 0 # counter for number samples accepted
  m <- 0 # counter for number of trials 
  samps   <- numeric(n.samps) # initialize the vector of output
  while (n < n.samps) {
    m <- m + 1
    u <- runif(1)
    y <- cauchy.sim(1, scale = 7, location = 66)
    if (u < f(y)/e(y)) {
      n <- n + 1
      samps[n] <- y
    }
  }
return(list(x=samps,alpha.hat=n.samps/m))
}
lapply(r.norm.mix(n.samps=10000), head, n=20)
alpha
```
2) (Solution) The proportion of accepted cases is 0.4377134 while the alpha that I chose is 0.44. Therefore, if the number of samples went to infinity, the proportion would go to alpha = 0.44.


3)  Using \textbf{ggplot} or \textbf{base R}, construct a histogram of the simulated mixture distribution with the true mixture pdf $f(x)$ overlayed on the plot. 

**Solution**
```{r}
x.vec <- seq(30,100,by=.1)
hist(x = r.norm.mix(n.samps=10000)$x, prob= TRUE, xlab = "x", 
     main = "Simulated Mixture Distribution w/ True Mixture pdf Overlay")
lines(x = x.vec, y = f(x.vec),col="purple")
   
```

## Part II: Bayesian Statistics and MCMC

Suppose that the experimenter collected 100 cases from the true mixture-normal distribution $f(x)$.  To solve problems (4) through (8) we analyze one realized sample from our function **r.norm.mix()**.  In practice this dataset would be collected and not simulated. Uncomment the below code to simulate our dataset **x**.  If you failed to solve Part I, then read in the csv file **mixture_data.csv** posted on Canvas.  

**Solution**

```{r}
# Simulate data
set.seed(1983)
x <- r.norm.mix(n.samps=100)$x
head(x)
hist(x,breaks=20,xlab="X",main="")

# Or read data 
#x <- read.csv("mixture_data.csv")$x
#head(x)
#hist(x,breaks=20,xlab="X",main="")
```


Further, suppose that we know the true heights and standard deviations of the two normal distributions but the mixture parameter $\delta$ is unknown. In this case, we know $\mu_1=70$, $\sigma_1=3$, $\mu_2=64$, $\sigma_2=2.5$.  The goal of this exercise is to utilize **maximum likelihood** and **MCMC Bayesian techniques** to estimate mixture parameter $\delta$.  


## Maximum likelihood Estimator of Mixture Parameter

4) Set up the likelihood function $L(\delta | x_1,\ldots,x_{100})$ and define it as **mix.like()**. The function should have two inputs including the parameter **delta** and data vector **x**.  Evaluate the likelihood at the parameter values **delta=.2**, **delta=.4,** and **delta=.6**.  Note that all three evaluations will be very small numbers.  Which delta ($\delta=.2,.4,.6$) is the most likely to have generated the dataset **x**?    
**Solution**

```{r}
mix.like <- function(x, delta) {
  return(sum(log(delta * dnorm(x,mean=70,sd=3) + (1-delta) * dnorm(x,mean=64,sd=2.5))))
}

mix.like(x, 0.2)
mix.like(x, 0.4)
mix.like(x, 0.6)
```

The most likely of the three is $\delta=.4$. 


5) Compute the maximum likelihood estimator of mixture parameter $\delta$. To accomplish this task, apply your likelihood function **mix.like()** across the vector **seq(.1,.99,by=.001)**.  The solution to this exercise is given below. 

```{r}
delta <- seq(.1,.99,by=.001)
MLE.values <- sapply(delta,mix.like,x=x)
delta.MLE <- delta[which.max(MLE.values)]
plot(delta,MLE.values,ylab="Likelihood",type="l")
abline(v=delta.MLE,col="blue")
text(x=delta.MLE+.08,y=mix.like(delta=.45,x=x),paste(delta.MLE),col="blue")
```


## MCMC


6) Run the Metropolis-Hastings algorithm to estimate mixture parameter $\delta$.  In this exercise you will assume a Beta($\alpha=10,\beta=10$) prior distribution on mixture parameter $\delta$. Some notes follow:  
\begin{itemize}
\item Run 20000 iterations. I.e., simulate 20000 draws of $\delta^{(t)}$  
\item Proposal distribution Beta($\alpha=10,\beta=10$)  
\item Independence chain with Metropolis-Hastings ratio:
\[
R(\delta^{(t)},\delta^*)=\frac{L(\delta^* | x_1,\ldots,x_{100})}{L(\delta^{(t)} | x_1,\ldots,x_{100})}
\]
\end{itemize}
Display the first 20 simulated  cases of $\delta^{(t)}$.  

**Solution**

```{r}
delta_1 <- rbeta(1, 10, 10)
n.samps <- 20000
delta_vec <- rep(NA, (n.samps + 1))
delta_vec[1] <- delta_1

for (t in 1:n.samps) {
  delta_star <- rbeta(1, 10, 10)
  delta_t <- delta_vec[t]
  
  MH_ratio <- prod(delta_star * dnorm(x,mean=70,sd=3) + (1-delta_star) 
                   * dnorm(x,mean=64,sd=2.5))/prod(delta_t 
                                                   * dnorm(x,mean=70,sd=3) + (1-delta_t)
                                                   * dnorm(x,mean=64,sd=2.5))
  
  prob_vec <- c(min(MH_ratio, 1), 1-min(MH_ratio, 1))
delta_vec[t+1] <- sample(c(delta_star, delta_t), 1, prob = prob_vec)
}
delta_vec[1:20]
```

7) Construct a lineplot of the simulated Markov chain from exercise (6). The vertical axis is the simulated chain $\delta^{(t)}$ and the horizontal axis is the number of iterations.  

**Solution**

```{r}
plot(x=1:20001, y=delta_vec, type="l")
```

8) Plot the empirical autocorrelation function of your simulated chain $\delta^{(t)}$.  I.e., run the function **acf()**.  A quick decay of the chain's autocorrelations indicate good mixing properties.     

**Solution**

```{r}
acf(delta_vec ,main="ACF: Prior Beta(10,10)")
```

9) Compute the empirical Bayes estimate $\hat{\delta}_B$ of the simulated posterior distribution $\pi(\delta|x_1,\ldots,x_n)$. To solve this problem, simply compute the sample mean of your simulated chain $\delta^{(t)}$ after discarding a 20\% burn-in.   

**Solution**

```{r}
burnin <- round(0.2 * length(delta_vec))
bayes.est <- sum(delta_vec[(burnin+1):length(delta_vec)])/(length(delta_vec) - burnin)
bayes.est
```

10) Construct a histogram of the simulated posterior $\pi(\delta|x_1,\ldots,x_n)$ after discarding a 20\% burn-in.  

**Solution**

```{r}
hist(delta_vec[(burnin+1):length(delta_vec)],
     main="Histogram of Simulated Posterior w/ Burn-in and Prior Beta(10, 10)",
     xlab="Simulated Value")
```

11) Run the Metropolis-Hastings algorithm to estimate the mixture parameter $\delta$ using a Beta($\alpha=15,\beta=2$) prior distribution on mixture parameter $\delta$. Repeat exercises 6 though 10 using the updated prior. This problem  
 
**Solution**

```{r}
new.delta_1 <- rbeta(1, 15, 2)
new.n.samps <- 20000
new.delta_vec <- rep(NA, (new.n.samps + 1))
new.delta_vec[1] <- new.delta_1

for (t in 1:new.n.samps) {
  new.delta_star <- rbeta(1, 15, 2)
  new.delta_t <- new.delta_vec[t]
  
  new.MH_ratio <- prod(new.delta_star * dnorm(x,mean=70,sd=3) + (1-new.delta_star) 
                       * dnorm(x,mean=64,sd=2.5))/prod(new.delta_t * dnorm(x,mean=70,sd=3) 
                                                       + (1-new.delta_t) * dnorm(x,mean=64,sd=2.5))
  
  new.prob_vec <- c(min(new.MH_ratio, 1), 1-min(new.MH_ratio, 1))
new.delta_vec[t+1] <- sample(c(new.delta_star, new.delta_t), 1, prob = new.prob_vec)
}
new.delta_vec[1:20]
```


**lineplot:** 

Construct a lineplot of the simulated Markov chain from exercise (6). The vertical axis is the simulated chain $\delta^{(t)}$ and the horizontal axis is the number of iterations. 

**Solution**

```{r}
plot(x=1:20001, y=new.delta_vec, type="l")
```

**ACF:** 

Plot the empirical autocorrelation function of your simulated chain $\delta^{(t)}$.  I.e., run the function **acf()**.  A slow decay of the chain's autocorrelations indicate poor mixing properties.    

**Solution**

```{r}
acf(new.delta_vec ,main="ACF: Prior Beta(15,2)")
```

**Bayes estimate:** 

Compute the empirical Bayes estimate $\hat{\delta}_B$ of the simulated posterior distribution $\pi(\delta|x_1,\ldots,x_n)$. To solve this problem, simply compute the sample mean of your simulated chain $\delta^{(t)}$ after discarding a 20\% burn-in. Your answer should be close to the MLE. 

**Solution**

```{r}
new.burnin <- round(0.2 * length(new.delta_vec))
new.bayes.est <- sum(new.delta_vec[(new.burnin+1):length(new.delta_vec)])/
  (length(new.delta_vec) - new.burnin)
new.bayes.est
```

**Posterior:**  Construct a histogram of the simulated posterior $\pi(\delta|x_1,\ldots,x_n)$ after discarding a 20\% burn-in.  


**Solution**

```{r}
hist(new.delta_vec[(new.burnin+1):length(new.delta_vec)], 
     main="Histogram of Simulated Posterior w/ Burn-in and Prior Beta(15, 2)", 
     xlab="Simulated Value")
```
